# Design 2.1: Stabilize E2E Test Suite

**Author:** AI Assistant
**Date:** 2025-04-30
**Status:** Draft

**Related Idea:** [Idea 2: Stabilize E2E Test Suite](../ideas/2.Stabilize-E2E-Test-Suite.md)

## 1. Goal

The primary goal of this design is to achieve a stable and reliable End-to-End (E2E) test suite using the `npm test e2e` command. Currently, the suite suffers from numerous failures (as documented in Idea 2) that hinder development velocity and reduce confidence in test results. This design outlines the steps to investigate these failures and implement fixes to make the E2E tests a dependable measure of application health. A secondary goal is to improve the overall execution speed of the test suite to make it suitable for integration into a CI/CD workflow.

## 2. Background & Current State

During the execution of Task T1.1.1, the Playwright E2E test suite (`npm test e2e`) revealed significant instability. Key issues observed include:

*   **Brittle Assertions:** Tests failing due to minor variations in AI-generated text.
*   **Configuration Errors:** Missing environment variables or configurations (e.g., `chat-model-reasoning`).
*   **Backend Errors:** HTTP 500 errors from backend endpoints like `/api/vote` (JSON parsing, database foreign key violations).
*   **Timeouts:** Tests exceeding the default timeout limits.

These failures are currently ignored when running tests for unrelated tasks (like T1.1.1), which is not sustainable.

## 3. Proposed Actions

1.  **Comprehensive Test Run & Analysis:**
    *   Execute the full E2E test suite (`npm test e2e`).
    *   Systematically document *every* failing test, capturing logs (frontend and backend if possible), error messages, and screenshots/videos provided by Playwright.
    *   Categorize failures (e.g., flaky assertion, config issue, backend bug, frontend bug, test logic bug).
2.  **Prioritization:**
    *   Prioritize fixing failures based on severity and frequency. Configuration errors and consistent backend bugs should likely be addressed first. Flaky tests might be tackled later or isolated.
3.  **Fix Implementation (Iterative):**
    *   Address failures category by category or test by test.
    *   For backend errors: Analyze backend logs, potentially debug backend services (`ai-relay-service`).
    *   For frontend errors/flakiness: Analyze frontend logs/behavior, adjust test logic, selectors, or assertions. Consider using more robust selectors or waiting strategies.
    *   For brittle AI assertions: Explore strategies like matching keywords/structure instead of exact text, or mocking AI responses for specific test scenarios.
    *   For timeouts: Investigate performance bottlenecks or increase timeouts where appropriate (with justification).
    *   While implementing fixes, consider the impact on test execution speed. Optimize selectors, waits, and test logic where possible without sacrificing reliability.
4.  **Refinement & Documentation:**
    *   Refactor test code for clarity and maintainability as fixes are implemented.
    *   Document fixes and any changes to test strategy or configuration.

## 4. Non-Goals

*   Adding *new* E2E test coverage (unless necessary to validate a fix).
*   Major refactoring of the application code itself (unless directly required to fix a test).
*   Implementing a completely new testing framework.

## 5. Future Work / Considerations

*   Setting up CI/CD integration for the E2E tests (dependent on achieving stability and acceptable execution speed).
*   Developing a strategy for handling AI response variability in tests long-term.
*   Performance testing (potentially beyond just the E2E suite execution time).

## 6. Task Breakdown

*   **[Task T2.1.1: Fix E2E Test Suite Failures](./tasks/T2.1.1-Fix-E2E-Failures.md)**: Systematically address the identified test failures and improve stability/speed. *(Status: To Do)* 